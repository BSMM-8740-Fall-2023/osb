---
title: "Omited variable bias"
---

```{r opts, include = FALSE}
options(width = 95)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

This fact will about covariance estimation be useful for the following discussion:

$$
\begin{align*}
\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) & =\sum_{i}\left(x_{i}-\bar{x}\right)y_{i}-\sum_{i}\left(x_{i}-\bar{x}\right)\bar{y}\\
 & =\sum_{i}\left(x_{i}-\bar{x}\right)y_{i}-\bar{y}\sum_{i}\left(x_{i}-\bar{x}\right)\\
 & =\sum_{i}\left(x_{i}-\bar{x}\right)y_{i}
\end{align*}
$$ and by the same argument $\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)=\sum_{i}\left(y_{i}-\bar{y}\right)x_{i}$ and $\sum_{i}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)=\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}$

------------------------------------------------------------------------

## OLR

Per our population model

$$
y = \beta_0 + \beta_1x + u
$$ and so our sample regression is

$$
y_i = \beta_0 + \beta_1x_i + u_i
$$ {#eq-population_reg} where the index $i$ identifies each sample, and our prediction is $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$. with residuals $u_i=y_i-\hat{y_i}$.

We know that the OLS formula for the regression coefficient $\beta_1$ is

$$
\begin{align*}
\hat{\beta_{1}} & =\frac{\text{Cov}\left(x_{i},y_{i}\right)}{\text{Var}\left(x_{i}\right)}\\
 & =\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)}\\
 & =\frac{\sum_{i}\left(x_{i}-\bar{x}\right)y_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}
\end{align*}
$$

but per our model @eq-population_reg we can write

$$
\begin{align*}
\hat{\beta_{1}} & =\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1}x_{i}+u_{i}\right)}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}\\
 & =\frac{\beta_{0}\sum_{i}\left(x_{i}-\bar{x}\right)+\beta_{1}\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}+\sum_{i}\left(x_{i}-\bar{x}\right)u_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}=\\
 & =\beta_{1}+\frac{\sum_{i}\left(x_{i}-\bar{x}\right)u_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}
\end{align*}
$$

and so, our estimate $\hat{\beta_1}$ is equal to the population parameter $\beta$ [**IF**]{.underline} the noise (i.e. residuals) is uncorrelated with our predictor.

The assumption that the residual term is uncorrelated with our predictor is one of the assumptions we used in setting up ordinary linear regression.

## Omitted variable bias (OVB)

But what if the residuals are not uncorrelated with the predictor? For example what if our population model was really

$$
y = \beta_0 + \beta_1x + \beta_1z + u
$$ with unobserved variable $z$, but our estimates are $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$, then

$$
\begin{align*}
\hat{\beta_{1}} & =\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1}x_{i}+z_{i}+u_{i}\right)}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}\\
 & =\frac{\beta_{0}\sum_{i}\left(x_{i}-\bar{x}\right)+\beta_{1}\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}+\beta_{2}\sum_{i}\left(x_{i}-\bar{x}\right)z_{i}+\sum_{i}\left(x_{i}-\bar{x}\right)u_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}=\\
 & =\beta_{1}+\beta_{2}\frac{\sum_{i}\left(x_{i}-\bar{x}\right)z_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}
\end{align*}
$$ and the bias in our estimate of $\beta_1$ is $\frac{\sum_{i}\left(x_{i}-\bar{x}\right)z_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right)x_{i}}$

## OVB in action:

In this example we will simulate what happens with linearly dependent predictors.

We have seen the data below in lab-4, where

-   $y = \text{demand1}$
-   $x = \text{price1},\;\beta_1=-0.5$
-   $z = \text{unobserved1},\;\beta_2=-1$

```{r}
#| echo: true
set.seed(1966)

dat1 <- tibble::tibble(
  unobserved1 = rnorm(500)
  , price1 = 10 + unobserved1 + rnorm(500)
  , demand1 = 23 -(0.5*price1 + unobserved1 + rnorm(500))
)
```

Without including the unobserved variable, the fit is

```{r}
fit1 <- lm(demand1 ~ price1, data = dat1)
broom::tidy(fit1)
```

and this fit estimates $\hat{\beta_1}=-0.99<-0.5$ so it is incorrect & biased. Checking using the covariance formula:

```{r}
cov(dat1$demand1, dat1$price1)/var(dat1$price1)
```

But we know we have an unobserved variable (and we know $\beta_2=-1$) so we can correct the bias.

```{r}
# biased estimate
biased_est <- broom::tidy(fit1) %>% 
  dplyr::filter(term == 'price1') %>% 
  dplyr::pull(estimate)

# bias
bias <- -cov(dat1$unobserved1, dat1$price1)/var(dat1$price1)

#corrected estimate
biased_est - bias
```

```{r}
lm(demand1 ~ price1 + unobserved1, data = dat1) %>% 
  broom::tidy()
```

------------------------------------------------------------------------

## **Frisch--Waugh--Lovell theorem**

### FWL or decomposition theorem:

When estimating a model of the form

$$
y = \beta_0 + \beta_1x_1 + \beta_1x_2 + u
$$

then the following estimators of $\beta_1$ are equivalent

-   the OLS estimator obtained by regressing $y$ on $x_1$ and $x_2$
-   the OLS estimator obtained by regressing $y$ on $\check{x}_1$
    -   where $\check{x}_1$ is the residual from the regression of $x_1$ on $x_2$
-   the OLS estimator obtained by regressing $\check{y}$ on $\check{x}_1$
    -   where $\check{y}$ is the residual from the regression of $y$ on $x_2$

### Interpretation:

The **Frisch-Waugh-Lowell** theorem is telling us that there are multiple ways to estimate a single regression coefficient. One possibility is to run the full regression of $y$ on $x$, as usual.

However, we can also regress $x_1$ on $x_2$, take the residuals, and regress $y$ only those residuals. The first part of this process is sometimes referred to as **partialling-out** (or *orthogonalization*, or *residualization*) of $x_1$ with respect to $x_2$. The idea is that we are isolating the variation in $x_1$ that is independent of (*orthogonal* to) $x_2$. Note that $x_2$ can be also be multi-dimensional (i.e. include multiple variables and not just one).

Why would one ever do that?

This seems like a way more complicated procedure. Instead of simply doing the regression in 1 step, now we need to do 2 or even 3 steps. It's not intuitive at all. The main advantage comes from the fact that we have reduced a multivariate regression to a univariate one, making more tractable and more intuitive.

### Example1

Using the data from OVB example:

```{r}
# partial out unobserved1 (predictor) from price1 (predictor)
fit_price <- lm(price1 ~ unobserved1, data = dat1)

# regress demand1 (outcome) on price residuals
lm(
  demand1 ~ price_resid
  , data = tibble::tibble(demand1 = dat1$demand1, price_resid = fit_price$residuals)
) %>% 
  broom::tidy()
```

```{r}
# partial out unobserved1 (predictor) from demand1 (outcome)
fit_demand <- lm(demand1 ~ unobserved1, data = dat1)

# regress demand resiuals on price residuals
lm(
  demand_resid ~ price_resid
  , data = tibble::tibble(demand_resid = fit_demand$residuals, price_resid = fit_price$residuals)
) %>% 
  broom::tidy()
```
