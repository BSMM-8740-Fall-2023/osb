---
title: "Regularized Regression: Ridge, Lasso and Elastic Net"
---

::: callout-note
The following supplemental notes are based on [this page](https://www.datacamp.com/tutorial/tutorial-ridge-lasso-elastic-net). They are provided for students who want to dive deeper into the mathematics behind regularized regression. Additional supplemental notes will be added throughout the semester.
:::

In the simple linear regression model, you have $n$ observations of the response variable $Y$ with a linear combination of $m$ predictor variables $X$ where

$$
\mathbb{P}\left(Y=y|\mathbf{\beta}\right)=\mathcal{N}\left(\left.y\right|\mathbf{\mathbf{\beta}},\sigma^2\right)
$$ {#eq-linear}

where c is a vector of parameters and $\sigma^2$ is the variance.

We must estimate the parameter values $\mathbf{\hat{\beta}}$ from the data, and using the OLS method, the loss function is

$$
\text{L}_{OLS}\left(\hat{\beta}\right)=\sum_{i=1}^{n}\left(y_{i}-x_{i}^{'}\hat{\beta}\right)^{2}=\left\Vert y-X\hat{\beta}\right\Vert ^{2}
$$ {#eq-OLS-loss}

which is minimized with the estimate

$$
\hat{\mathbf{\beta}}_{OLS}=\left(X'X\right)^{-1}\left(X'Y\right)
$$ {#eq-OLS-soln}

### Bias and variance

The bias is the difference between the true population parameter and the expected estimator:

$$
\text{Bias}\left(\hat{\mathbf{\beta}}_{OLS}\right)=\mathbb{E\left[\hat{\mathbf{\beta}}_{OLS}\right]-\mathbf{\beta}}
$$ {#eq-OLS-bias}

The variance uncertainty, in these estimates:

$$
\text{Var}\left(\hat{\mathbf{\beta}}_{OLS}\right)=\sigma^{2}\left(X'X\right)^{-1}
$$ {#eq-OLS-variance}

where $\sigma^2$ is estimated form the residuals $e$

$$
\begin{align*}e & =y-x\hat{\mathbf{\beta}}\\\hat{\sigma}^{2} & =\frac{e'e}{n-m}\end{align*}
$$ {#eq-OLS-residuals}

This picture illustrates what bias and variance are.

![Source: kdnuggets.com](/images/bias_vs_variance_swxhxx.jpg){fig-alt="Source: kdnuggets.com" fig-align="center"}

Both the bias and the variance are desired to be low, and the model's error can be decomposed into three parts: error resulting from a large variance, error resulting from significant bias, and the remainder - the unexplainable part.

$$
\begin{align*}\mathbb{E}\left[e\right] & =\left(\mathbb{E}\left[X\hat{\beta}\right]-X\hat{\beta}\right)^{2}+\mathbb{E}\left[\left(X\hat{\beta}-X\hat{\beta}\right)^{2}\right]+\sigma^{2}\\ & =\text{Bias}^{2}+\text{Variance}+\sigma^{2}\end{align*}
$$ {#eq-OLS-expected-error}

The OLS estimator has the desired property of being unbiased. However, it can have a huge variance. Specifically, this happens when:

-   The predictor variables are highly correlated with each other;
-   There are many predictors. This is reflected in the formula for variance given above: if *m* approaches *n*, the variance approaches infinity.

The general solution to this is: **reduce variance at the cost of introducing some bias**. This approach is called regularization and is almost always beneficial for the predictive performance of the model. To make it sink in, let's take a look at the following plot.

![Source: researchgate.net](/images/tradeoff_sevifm.png){fig-alt="Source: researchgate.net" fig-align="center"}

As the model complexity, which in the case of linear regression can be thought of as the number of predictors, increases, estimates' variance also increases, but the bias decreases. The unbiased OLS would place us on the right-hand side of the picture, which is far from optimal. That's why we regularize: to lower the variance at the cost of some bias, thus moving left on the plot, towards the optimum.

## Ridge Regression

From the discussion so far we have concluded that we would like to decrease the model complexity, that is the number of predictors. We could use the forward or backward selection for this, but that way we would not be able to tell anything about the removed variables' effect on the response. Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, let's penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model. This, basically, is what Ridge Regression does.

### Model Specification

In Ridge Regression, the OLS loss function is augmented in such a way that we not only minimize the sum of squared residuals but also penalize the size of parameter estimates, in order to shrink them towards zero:

$$
\text{L}_{ridge}\left(\hat{\beta}\right)=\sum_{i=1}^{n}\left(y_{i}-x_{i}^{'}\hat{\beta}\right)^{2} + \lambda\sum_{j=1}^{m}\hat{\beta}^2_j=\left\Vert y-X\hat{\beta}\right\Vert ^{2} + \lambda\left\Vert \hat{\beta}\right\Vert ^{2}
$$

Solving this for $\hat\beta$ gives the the ridge regression estimates $\hat\beta_{ridge} = (X'X+\lambda I)^{-1}(X'Y)$, where I denotes the identity matrix.

The $\lambda$ parameter is the regularization penalty. We will talk about how to choose it in the next sections of this tutorial, but for now notice that:

-   As $\lambda \rightarrow 0, \quad \hat\beta_{ridge} \rightarrow \hat\beta_{OLS}$;
-   As $\lambda \rightarrow \infty, \quad \hat\beta_{ridge} \rightarrow 0$.

So, setting $\lambda$ to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients' size penalized.

### Bias-Variance Trade-Off in Ridge Regression

$$
\text{Bias}\left(\hat{\mathbf{\beta}}_{ridge}\right) &=\lambda (X'X+\lambda I)^{-1}\beta\\
\text{Var}\left(\hat{\mathbf{\beta}}_{ridge}\right) &= \sigma^{2}(X'X+\lambda I)^{-1} X'X (X'X+\lambda I)^{-1}
$$

This document contains the mathematical details for deriving the least-squares estimates for slope ($\beta_1$) and intercept ($\beta_0$). We obtain the estimates, $\hat{\beta}_1$ and $\hat{\beta}_0$ by finding the values that minimize the sum of squared residuals, as shown in @eq-ssr.

$$
SSR = \sum\limits_{i=1}^{n}[y_i - \hat{y}_i]^2 = [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)]^2 = [y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i]^2
$$ {#eq-ssr}

Recall that we can find the values of $\hat{\beta}_1$ and $\hat{\beta}_0$ that minimize /eq-ssr by taking the partial derivatives of @eq-ssr and setting them to 0. Thus, the values of $\hat{\beta}_1$ and $\hat{\beta}_0$ that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in @eq-par-deriv.

$$
\begin{aligned}
\frac{\partial \text{SSR}}{\partial \hat{\beta}_1} &= -2 \sum\limits_{i=1}^{n}x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)  \\
\frac{\partial \text{SSR}}{\partial \hat{\beta}_0} &= -2 \sum\limits_{i=1}^{n}(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)
\end{aligned}
$$ {#eq-par-deriv}

The derivation of deriving $\hat{\beta}_0$ is shown in @eq-est-beta0.

$$
\begin{aligned}\frac{\partial \text{SSR}}{\partial \hat{\beta}_0} &= -2 \sum\limits_{i=1}^{n}(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\&\Rightarrow -\sum\limits_{i=1}^{n}(y_i + \hat{\beta}_0 + \hat{\beta}_1 x_i) = 0 \\&\Rightarrow - \sum\limits_{i=1}^{n}y_i + n\hat{\beta}_0 + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i = 0 \\&\Rightarrow n\hat{\beta}_0  = \sum\limits_{i=1}^{n}y_i - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i \\&\Rightarrow \hat{\beta}_0  = \frac{1}{n}\Big(\sum\limits_{i=1}^{n}y_i - \hat{\beta}_1\sum\limits_{i=1}^{n}x_i\Big)\\&\Rightarrow \hat{\beta}_0  = \bar{y} - \hat{\beta}_1 \bar{x} \\\end{aligned}
$$ {#eq-est-beta0}

The derivation of $\hat{\beta}_1$ using the $\hat{\beta}_0$ we just derived is shown in @eq-est-beta1-pt1.

$$
\begin{aligned}&\frac{\partial \text{SSR}}{\partial \hat{\beta}_1} = -2 \sum\limits_{i=1}^{n}x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0  \\&\Rightarrow -\sum\limits_{i=1}^{n}x_iy_i + \hat{\beta}_0\sum\limits_{i=1}^{n}x_i + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = 0 \\\text{(Fill in }\hat{\beta}_0\text{)}&\Rightarrow -\sum\limits_{i=1}^{n}x_iy_i + (\bar{y} - \hat{\beta}_1\bar{x})\sum\limits_{i=1}^{n}x_i + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = 0 \\&\Rightarrow  (\bar{y} - \hat{\beta}_1\bar{x})\sum\limits_{i=1}^{n}x_i + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = \sum\limits_{i=1}^{n}x_iy_i \\&\Rightarrow \bar{y}\sum\limits_{i=1}^{n}x_i - \hat{\beta}_1\bar{x}\sum\limits_{i=1}^{n}x_i + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = \sum\limits_{i=1}^{n}x_iy_i \\&\Rightarrow n\bar{y}\bar{x} - \hat{\beta}_1n\bar{x}^2 + \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 = \sum\limits_{i=1}^{n}x_iy_i \\&\Rightarrow \hat{\beta}_1\sum\limits_{i=1}^{n}x_i^2 - \hat{\beta}_1n\bar{x}^2  = \sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x} \\&\Rightarrow \hat{\beta}_1\Big(\sum\limits_{i=1}^{n}x_i^2 -n\bar{x}^2\Big)  = \sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x} \\ &\hat{\beta}_1 = \frac{\sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x}}{\sum\limits_{i=1}^{n}x_i^2 -n\bar{x}^2}\end{aligned}
$$ {#eq-est-beta1-pt1}

To write $\hat{\beta}_1$ in a form that's more recognizable, we will use the following:

$$
\sum x_iy_i - n\bar{y}\bar{x} = \sum(x - \bar{x})(y - \bar{y}) = (n-1)\text{Cov}(x,y)
$$ {#eq-cov}

$$
\sum x_i^2 - n\bar{x}^2 - \sum(x - \bar{x})^2 = (n-1)s_x^2
$$ {#eq-var_x}

where $\text{Cov}(x,y)$ is the covariance of $x$ and $y$, and $s_x^2$ is the sample variance of $x$ ($s_x$ is the sample standard deviation).

Thus, applying @eq-cov and @eq-var_x, we have

$$
\begin{aligned}\hat{\beta}_1 &= \frac{\sum\limits_{i=1}^{n}x_iy_i - n\bar{y}\bar{x}}{\sum\limits_{i=1}^{n}x_i^2 -n\bar{x}^2} \\&= \frac{\sum\limits_{i=1}^{n}(x-\bar{x})(y-\bar{y})}{\sum\limits_{i=1}^{n}(x-\bar{x})^2}\\&= \frac{(n-1)\text{Cov}(x,y)}{(n-1)s_x^2}\\&= \frac{\text{Cov}(x,y)}{s_x^2}\end{aligned}
$$ {#eq-est-beta1-pt2}

The correlation between $x$ and $y$ is $r = \frac{\text{Cov}(x,y)}{s_x s_y}$. Thus, $\text{Cov}(x,y) = r s_xs_y$. Plugging this into @eq-est-beta1-pt2, we have

$$
\hat{\beta}_1 = \frac{\text{Cov}(x,y)}{s_x^2} = r\frac{s_ys_x}{s_x^2} = r\frac{s_y}{s_x}
$$ {#eq-est-beta1}
