---
title: "EDA & Feature Engineering"
subtitle: "BSMM-8740 - Fall 2023"
author: "L.L. Odette"
footer:  "[bsmm-8740-fall-2023.github.io/osb](https://bsmm-8740-fall-2023.github.io/osb/)"
logo: "images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 90)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```

## Today's Outline

-   Introduction to exploratory data analysis (EDA)
-   Feature engineering in the tidyverse

# Exploratory Data Analysis (EDA)

## Exploratory Data Analysis (EDA)

We are going to perform two broad categories of EDA:

-   Descriptive Statistics, which includes mean, median, mode, inter-quartile range, and so on.
-   Graphical Methods, which includes histogram, density estimation, box plots, and so on.

1.  Import the data
2.  Clean the data
3.  Process the data
4.  Visualize the data

## Exploratory Data Analysis (EDA)

::: {style="font-size: 32px"}
```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1|2"
dat <- forcats::gss_cat
dat %>% dplyr::slice_head(n=10) %>% dplyr::glimpse()
```
:::

## `DataExplorer::introduce`

The function `DataExplorer::introduce` produces a description of the data in a `data.frame`.

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::introduce() %>% dplyr::glimpse()
```

## `DataExplorer::plot_intro`

The function `DataExplorer::plot_intro` is a visual version of `DataExplorer::introduce`.

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_intro()
```

## `DataExplorer::plot_missing`

The function `DataExplorer::plot_missing` shows information on missing data visually.

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_missing()
```

## `D_Explorer::profile_missing`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1|2|3|4|5"
dat %>% DataExplorer::profile_missing() %>% 
  gt::gt('feature') %>% 
  gtExtras::gt_theme_espn() %>% 
  gt::tab_options( table.font.size = gt::px(28) ) %>% 
  gt::as_raw_html()
```

## `DataExplorer::plot_density`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "2|3"
dat %>% 
  DataExplorer::plot_density(
    ggtheme = theme_bw(base_size = 18) + theme(legend.position = "top")
  )
```

## `D_Explorer::plot_histogram`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "2|3"
dat %>% 
  DataExplorer::plot_histogram(
    ggtheme = theme_bw(base_size = 18) + theme(legend.position = "top")
  )
```

## `DataExplorer::plot_bar`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_bar()

```

## `D_Explorer::plot_correlation`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_correlation()
```

## `D_Explorer::plot_correlation`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
dat %>% DataExplorer::plot_correlation( maxcat = 5)
```

## `D_Explorer::plot_scatterplot`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1-2|3-5"
dat %>% 
  dplyr::select(year,age,tvhours) %>% 
  DataExplorer::plot_scatterplot(
    ggtheme = theme_bw(base_size = 18) + theme(legend.position = "top")
    , by = 'year', nrow=5
  )
```

## `DataExplorer::plot_prcomp`

```{r}
#| echo: false
#| warning: false
#| message: false
#| code-fold: true
#| output: false
#| code-line-numbers: "1|2-5"
prcomp_result <- na.omit(airquality) %>% 
  DataExplorer::plot_prcomp(
    ggtheme = theme_bw(base_size = 18) + theme(legend.position = "top")
    ,nrow = 2L, ncol = 2L, variance_cap=100
  ) 
```

```{r}
prcomp_result %>% 
  purrr::pluck("page_0")
```

## `DataExplorer::plot_prcomp`

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
prcomp_result %>% 
  purrr::pluck("page_1")
```

## Exploratory Data Analysis (EDA)

### Post-model development

-   confirm assumptions of the model (e.g. Normality)

-   understand origin of any systemic issues

# Feature engineering

Feature engineering is the act of converting raw observations into desired features using statistical or machine learning approaches.

## Feature engineering

Continuous variables:

-   Missing data
-   Normalization
-   Standardization
-   transformation (arithmetic, basis functions, polynomials, splines
-   outliers
-   lag variables
-   profiling

## Feature engineering

Categorical variables:

-   Label encoding
-   One-hot-encoding
-   imbalances

Date variables:

-   Dates, timestamps, intervals

## Feature engineering

Feature selection

Feature importance

Convert to factors

Add and Delete a column

Recode continuous variable to a categorical variable (rank, q-tiles, winsorizing, capping

Find missing values

Handle Missing values

PCA, IDA

## Feature engineering & selection

### Important concepts

-   Overfitting (to both data and predictors)
-   Supervised and Unsupervised Procedures
-   No Free Lunch Theorem
-   The Model versus the Modeling Process
-   Model Bias and Variance (in modelling context)
-   Data-driven vs experience-driven

## Feature engineering & selection

Categorical variables

-   dummy variables for un-ordered categories

-   encoding for many categories

    -   \# categories \>\> \# rows
    -   zero-variance predictors
    -   hashing

-   new categories

## Feature engineering & selection

Categorical variables

-   supervised encoding

    -   likelihood encoding (e.g. log-odds, embeddings, NN)

-   encoding for ordered categories (e.g. polynomial contrast)

Features from text

Factors vs. dummy vars in tree models

## Feature engineering & selection

![](/images/category_fail.png)

## Feature engineering & selection

Numeric variables: issues

-   different scales
-   skewness and extreme values
-   censored data
-   complex functional relation to target
-   rednundant

## Feature engineering & selection

Transformations:

-   Box-cox: with $\tilde{x}$ the geometric mean of the (positive) predictor data

$$
x^{*} = \left\{ 
\begin{array}{cc}
\frac{x^{\lambda}-1}{\lambda\tilde{x}^{\lambda-1}} & \lambda\ne 0\\
\tilde{x}\log x & \lambda=0
\end{array}
\right .  
$$

## Feature engineering & selection

Transformations:

-   logit transformation for bounded target variables (scaled to lie in $[0,1]$)

$$
\text{logit}\left(x\right)=\frac{x}{1-x}
$$

-   centering and scaling

## Feature engineering & selection

Transformations: when time is a predictor

-   data smoothing (target or predictors)

    -   running mean/median
    -   smoothing splines
    -   lags

## Feature engineering & selection

Transformations: 1:many

-   orthogonal basis expansions
    -   polynomial, fourier
-   binning / discretization

## Feature engineering & selection

Transformations: many:many

-   linear projection: for predictor matrix $X$, create score matrix $X^*=XA$
    -   principal component analysis: capturing variation in a smaller number of features/scores
        -   scores are uncorrelated

## Feature engineering & selection

Transformations: many:many

-   non-linear projection;
    -   kernel principle components (when target is a nonlinear function of predictors
-   independent component analysis
-   non-negative factorization
-   partial least-squares

## Feature engineering & selection

::: columns
::: {.column width="25%"}
-   autoencoders
:::

::: {.column width="75%"}
![](/images/ae.png){width="796"}
:::
:::

## Feature engineering & selection

Missing data

Why?

-   structural deficiencies in the data
-   random occurrences (MCAR & MAR)
-   specific causes
-   censored

## Feature engineering & selection

Solutions:

-   delete
-   encode
-   impute

## Feature engineering & selection

Profile data

-   experimental unit and unit of prediction
