---
title: "Design Matrices with Recipes"
subtitle: "BSMM-8740 - Fall 2023"
author: "derived from Max Kuhn (RStudio)"
footer:  "[bsmm-8740-fall-2023.github.io/osb](https://bsmm-8740-fall-2023.github.io/osb/)"
logo: "images/logo.png"
title-slide-attributes:
  data-background-image: images/my-DRAFT.png
  data-background-size: contain
  data-background-opacity: "0.40"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
editor: visual
menu:
  numbers: true
execute:
  freeze: auto
---

```{r opts, include = FALSE}
options(width = 90)
library(knitr)
opts_chunk$set(comment="", 
               digits = 3, 
               tidy = FALSE, 
               prompt = TRUE,
               fig.align = 'center')
require(magrittr)
require(ggplot2)
require(mlbench)
data("PimaIndiansDiabetes")
library(caret)
data("Sacramento")
library(recipes)
library(lattice)
theme_set(theme_bw() + theme(legend.position = "top"))
```

## Recap of last lecture

Last time we worked with exploratory data analysis and then used the tidyverse verbs to engineer features into our datasets.

## Today's Outline

-   Introduction to data pre-processing and design/model matrix generation with the `Recipes` package.
-   Using recipes will facilitate developing feature engineering workflows that can be applied to multiple datasets (e.g. train and test as well as cross-validation sets)

## R Model Formulas

A simple formula in a linear model to predict house prices:

```{r sac}
#| echo: true
#| message: false
#| results: false
#| code-line-numbers: "1|2|3|4"
mod1 <- stats::lm(
  log(price) ~ type + sqft
  , data = Sacramento
  , subset = beds > 2
  )
```

::: {style="font-size: 80%"}
The purpose of this code chunk:

1.  subset some of the data points (`subset`)
2.  create a design matrix for 2 predictor variable (but 3 model terms)
3.  log transform the outcome variable
4.  fit a linear regression model
:::

The first two steps create the *design matrix*.

## Example design matrix

```{r}
#| echo: true
#| message: false
#| results: false
#| code-line-numbers: "1|2|3"
 mm <- model.matrix(
   log(price) ~ type + sqft
   , data = Sacramento
)
```

```{r}
#| echo: false
mm %>% head(14)
```

## Summary: Model Formula Method

-   Model formulas are very expressive in that they can represent model terms easily
-   The formula/terms framework does some elegant functional programming
-   Functions can be embedded inline to do fairly complex things (on single variables) and these can be applied to new data sets.

## Model formula examples

::: panel-tabset
## leave out

```{r}
#| echo: true
model.matrix(log(price) ~ -1 + type + sqft, data = Sacramento) %>% head()
```

## interaction

```{r}
#| echo: true
model.matrix(log(price) ~ type : sqft, data = Sacramento) %>% head()
```

## crossing

```{r}
#| echo: true
model.matrix(log(price) ~ type * sqft, data = Sacramento) %>% head()
```

## nesting

```{r}
#| echo: true
model.matrix(log(price) ~ type %in% sqft, data = Sacramento) %>% head()
```

## as-is

```{r}
#| echo: true
model.matrix(log(price) ~ type + sqft + I(sqft^2), data = Sacramento) %>% head()
```

contrast with log(price) \~ type + sqft + sqft\^2
:::

## Summary: Model Formula Method

There are significant limitations to what this framework can do and, in some cases, it can be very inefficient.

This is mostly due of being written well before large scale modeling and machine learning were commonplace.

## Limitations of the Current System

-   Formulas are not very extensible especially with nested or sequential operations (e.g. `y ~ scale(center(knn_impute(x)))`).
-   When used in modeling functions, you cannot recycle the previous computations.
-   For wide data sets, the formula method can be very inefficient and consume a significant proportion of the total execution time.

## Limitations of the Current System

-   Multivariate outcomes are kludgy by requiring `cbind`
-   Formulas have a limited set of roles (next two slides)

A more in-depth discussion of these issues can be found in [this blog post](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/).

## Variable Roles

Formulas have been re-implemented in different packages for a variety of different reasons:

```{r roles_1}
#| echo: true
#| message: false
#| results: false
#| eval: false
#| code-line-numbers: "3|8-10|14-16"
# ?lme4::lmer
# Subjects need to be in the data but are not part of the model
lme4::lmer(Reaction ~ Days + (Days | Subject), data = lme4::sleepstudy)

# BradleyTerry2
# We want to make the outcomes to be a function of a 
# competitor-specific function of reach 
BradleyTerry2::BTm(outcome = 1, player1 = winner, player2 = loser,
    formula = ~ reach[..] + (1|..), 
    data = boxers)

# modeltools::ModelEnvFormula (using the modeltools package for formulas)
# mob
data(PimaIndiansDiabetes, package = 'mlbench')
modeltools::ModelEnvFormula(diabetes ~ glucose | pregnant + mass +  age,
    data = PimaIndiansDiabetes)

```

## Variable Roles

A general list of possible variable roles could be:

::: {style="font-size: 75%"}
-   outcomes
-   predictors
-   stratification
-   model performance data (e.g. loan amount to compute expected loss)
-   conditioning or faceting variables (e.g. [`lattice`](https://cran.r-project.org/package=lattice) or [`ggplot2`](https://cran.r-project.org/package=ggplot2))
-   random effects or hierarchical model ID variables
-   case weights (\*)
-   offsets (\*)
-   error terms (limited to `Error` in the `aov` function)(\*)

(\*) Can be handled in formulas but are hard-coded into functions.
:::

## Recipes

We can approach the design matrix and preprocessing steps by first specifying a **sequence of steps**.

1.  `price` is an outcome
2.  `type` and `sqft` are predictors
3.  log transform `price`
4.  convert `type` to dummy variables

## Recipes

A recipe is a specification of *intent*.

One issue with the formula method is that it couples the specification for your predictors along with the implementation.

Recipes separate the *planning* from the *doing*.

Website: [`https://topepo.github.io/recipes`](https://topepo.github.io/recipes)

## Recipes

A *recipe* can be trained then applied to any data.

```{r rec_basic}
#| echo: true
#| message: false
#| code-line-numbers: "2|4-6|9|11"
## Create an initial recipe with only predictors and outcome
rec <- recipes::recipe(price ~ type + sqft, data = Sacramento)

rec <- rec %>% 
  recipes::step_log(price) %>%
  recipes::step_dummy(type)

# estimate any parameters
rec_trained <- recipes::prep(rec, training = Sacramento, retain = TRUE)
# apply the computations to new_data
design_mat  <- recipes::bake(rec_trained, new_data = Sacramento)
```

## Selecting Variables

In the last slide, we used `dplyr`-like syntax for selecting variables such as `step_dummy(type)`.

In some cases, the names of the predictors may not be known at the time when you construct a recipe (or model formula). For example:

-   dummy variable columns
-   PCA feature extraction when you keep components that capture $X$% of the variability.
-   discretized predictors with dynamic bins

## Selecting Variables

`dplyr` selectors can also be used on variables names, such as

```{r step_match}
#| echo: true
#| message: false
#| eval: false
rec %>% 
  recipes::step_spatialsign(
    matches("^PC[1-9]")
    , all_numeric()
    , -all_outcomes()
  )
```

Variables can be selected by name, role, data type, or any [combination of these](https://recipes.tidymodels.org/articles/Selecting_Variables.html).

## Example

::: panel-tabset
## data

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "1|4-5|8-9"
data(biomass, package = "modeldata")

# training set
biomass_tr <- biomass %>% tibble::as_tibble() %>% 
  dplyr::filter(dataset == 'Training')

# testing set
biomass_te <- biomass %>% tibble::as_tibble() %>% 
  dplyr::filter(dataset == 'Testing')
```

## recipe

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "1-4|6-9|11|13"
rec <- recipe(
  HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur,
  data = biomass_tr
)

ss_trans <- rec %>%
  step_center(carbon, hydrogen) %>%
  step_scale(carbon, hydrogen) %>%
  step_spatialsign(carbon, hydrogen)
# estimate any parameters
ss_obj <- prep(ss_trans, training = biomass_tr)
# apply the computations to new_data
transformed_te <- bake(ss_obj, biomass_te)
```

## results

```{r}
transformed_te
```

## use case

```{r}
#| echo: true
#| warning: false
#| message: false
#| code-fold: true
#| code-line-numbers: "1-4|6-12|14-17|19-26"
#| layout-ncol: 2
#| fig-subcap: 
#|   - "Raw data, with outliers"
#|   - "Transformed data"
set.seed(1); n <- 10000
tmp <- tibble::tibble(
  x = c(rnorm(n, 0, 0.02), -1, 1, 0.5)
  , y = c(rnorm(n, 0, 0.2), -1, 1, -2))

tmp %>% dplyr::slice_head(n=-3) %>%
  ggplot(aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(data=(tmp %>% dplyr::slice_tail(n=3))[1,], color="red", size=5) +
  geom_point(data=(tmp %>% dplyr::slice_tail(n=2))[1,], color="blue", size=5) +
  geom_point(data=(tmp %>% dplyr::slice_tail(n=1))[1,], color="green", size=5) +
  theme_bw(base_size = 32)

rec <- recipe(y ~ x, data = tmp) %>% 
  step_spatialsign(y, x) %>% 
  prep(training = tmp) %>% 
  bake(tmp) 

rec %>% 
  dplyr::slice_head(n=-3) %>%
  ggplot(aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(data=(rec %>% dplyr::slice_tail(n=3))[1,], color="red", size=5) +
  geom_point(data=(rec %>% dplyr::slice_tail(n=2))[1,], color="blue", size=5) +
  geom_point(data=(rec %>% dplyr::slice_tail(n=1))[1,], color="green", size=5) +
  theme_bw(base_size = 32)
```
:::

## Available Steps

-   **Basic**: [logs](https://recipes.tidymodels.org/reference/step_log.html), [roots](https://recipes.tidymodels.org/reference/step_sqrt.html), [polynomials](https://recipes.tidymodels.org/reference/step_poly.html), [logits](https://recipes.tidymodels.org/reference/step_logit.html), [hyperbolics](https://recipes.tidymodels.org/reference/step_hyperbolic.html)
-   **Encodings**: [dummy variables](https://recipes.tidymodels.org/reference/step_dummy.html), ["other"](https://recipes.tidymodels.org/reference/step_other.html) factor level collapsing, [discretization](https://recipes.tidymodels.org/reference/discretize.html)
-   **Date Features**: Encodings for [day/doy/month](https://recipes.tidymodels.org/reference/step_date.html) etc, [holiday indicators](https://recipes.tidymodels.org/reference/step_holiday.html)
-   **Filters**: [correlation](https://recipes.tidymodels.org/reference/step_corr.html), [near-zero variables](https://recipes.tidymodels.org/reference/step_nzv.html), [linear dependencies](https://recipes.tidymodels.org/reference/step_lincomb.html)
-   **Imputation**: [*K*-nearest neighbors](https://recipes.tidymodels.org/reference/step_knnimpute.html), [bagged trees](https://recipes.tidymodels.org/reference/step_bagimpute.html), [mean](https://recipes.tidymodels.org/reference/step_meanimpute.html)/[mode](https://recipes.tidymodels.org/reference/step_modeimpute.html) imputation,

## Available Steps

-   **Normalization/Transformations**: [center](https://recipes.tidymodels.org/reference/step_center.html), [scale](https://recipes.tidymodels.org/reference/step_scale.html), [range](https://recipes.tidymodels.org/reference/step_range.html), [Box-Cox](https://recipes.tidymodels.org/reference/step_BoxCox.html), [Yeo-Johnson](https://recipes.tidymodels.org/reference/step_YeoJohnson.html)
-   **Dimension Reduction**: [PCA](https://recipes.tidymodels.org/reference/step_pca.html), [kernel PCA](https://recipes.tidymodels.org/reference/step_kpca.html), [ICA](https://recipes.tidymodels.org/reference/step_ica.html), [Isomap](https://recipes.tidymodels.org/reference/step_isomap.html), [data depth](https://recipes.tidymodels.org/reference/step_depth.html) features, [class distances](https://recipes.tidymodels.org/reference/step_classdist.html)
-   **Others**: [spline basis functions](https://recipes.tidymodels.org/reference/step_ns.html), [interactions](https://recipes.tidymodels.org/reference/step_interact.html), [spatial sign](https://recipes.tidymodels.org/reference/step_spatialsign.html)

More on the way (i.e. autoencoders, more imputation methods, etc.)

One of the [package vignettes](https://www.tidymodels.org/learn/develop/recipes/) shows how to write your own step functions.

## Extending

Need to add more pre-processing or other operations?

```{r rec_add}
#| echo: true
#| message: false
#| results: false
standardized <- rec_trained %>%
  recipes::step_center(recipes::all_numeric()) %>%
  recipes::step_scale(recipes::all_numeric()) %>%
  recipes::step_pca(recipes::all_numeric())
          
## Only estimate the new parts:
standardized <- recipes::prep(standardized)
```

If an initial step is computationally expensive, you don't have to redo those operations to add more.

## Extending

Recipes can also be created with different roles manually

```{r rec_man, eval = FALSE}
#| echo: true
#| message: false
#| code-line-numbers: "2|3|4|5"
rec <- 
  recipes::recipe(x  = Sacramento) %>%
  recipes::update_role(price, new_role = "outcome") %>%
  recipes::update_role(type, sqft, new_role = "predictor") %>%
  recipes::update_role(zip, new_role = "strata")
```

Also, the sequential nature of steps means that there don't have to be R operations and could call other compute engines (e.g. Weka, scikit-learn, Tensorflow, etc. )

## Extending

We can create wrappers to work with recipes too:

```{r lm}
#| echo: true
#| message: false
#| code-line-numbers: "2|3-6"
lin_reg.recipe <- function(rec, data, ...) {
  trained <- recipes::prep(rec, training = data)
  lm.fit(
    x = recipes::bake(trained, newdata = data, all_predictors())
    , y = recipes::bake(trained, newdata = data, all_outcomes())
    , ...
  )
}
```

## An Example

[Kuhn and Johnson](http://appliedpredictivemodeling.com) (2013) analyze a data set where thousands of cells are determined to be well-segmented (WS) or poorly segmented (PS) based on 58 image features. We would like to make predictions of the segmentation quality based on these features.

## An Example

The `segmentationData` dataset has 61 columns

```{r image_load}
#| echo: true
#| message: false
#| code-line-numbers: "1|3-5|7-9"
data(segmentationData, package = "caret")

seg_train <- segmentationData %>% 
  dplyr::filter(Case == "Train") %>% 
  dplyr::select(-Case, -Cell)

seg_test  <- segmentationData %>% 
  dplyr::filter(Case == "Test")  %>% 
  dplyr::select(-Case, -Cell)
```

## A Simple Recipe

```{r image_rec}
#| echo: true
#| message: false
#| code-line-numbers: "1|3-8|10-17"
rec <- recipes::recipe(Class  ~ ., data = seg_train)

basic <- rec %>%
  # Correct some predictors for skewness
  recipes::step_YeoJohnson(recipes::all_predictors()) %>%
  # Standardize the values
  recipes::step_center(recipes::all_predictors()) %>%
  recipes::step_scale(recipes::all_predictors())

# Estimate the transformation and standardization parameters 
basic <- 
  recipes::prep(
    basic
    , training = seg_train
    , verbose = FALSE
    , retain = TRUE
  )  
```

## Principal Component Analysis[^1]

[^1]: A pretty good description of PCA can be found [here](https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd).

```{r image_pca}
#| echo: true
#| message: false
#| code-fold: true
#| code-line-numbers: "1|3-8|10-17"
pca <- basic %>% 
  recipes::step_pca(
    recipes::all_predictors()
    , threshold = .9
  )
```

```{r}
#| echo: false
summary(pca)
```

## Principal Component Analysis

```{r image_pca_train}
#| echo: true
#| message: false
pca %<>% recipes::prep() 
```

```{r}
#| echo: false
pca %>% summary() %>% dplyr::slice_head(n=12)
```

## Principal Component Analysis

```{r}
#| echo: true
#| message: false
#| code-line-numbers: "2|3|4"
pca %<>% 
  recipes::bake(
    new_data = seg_test
    , everything()
  )
pca[1:4, 1:8]
```

## Principal Component Analysis

```{r image_pca_plot}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "PCA1"
#| width: "80%"
pca %>% ggplot(aes(x = PC01, y = PC02, color = Class)) + 
  geom_point(alpha = .4) +
  theme_bw(base_size = 25)
```

## Kernel Principal Component Analysis

```{r kpca}
#| echo: true
#| message: false
#| code-line-numbers: "2|3-9|12|13"
kern_pca <- basic %>% 
  recipes::step_kpca(
    recipes::all_predictors()
    , num_comp = 2
    , options = 
      list(
        kernel = "rbfdot"
        , kpar = list(sigma = 0.05)
      )
  )

kern_pca <- recipes::prep(kern_pca)
kern_pca <- recipes::bake(kern_pca, new_data = seg_test, everything())
```

## Kernel Principal Component Analysis

```{r image_kpca_fig}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Kernel PCA"
#| width: "80%"
rngs <- grDevices::extendrange(c(kern_pca$kPC1, kern_pca$kPC2))
kern_pca %>% ggplot(aes(x = kPC1, y = kPC2, color = Class)) + 
  geom_point(alpha = .4) + 
  xlim(rngs) + ylim(rngs) + 
  # theme(legend.position = "top") +
  theme_bw(base_size = 25)
```

## Distance to Each Class Centroid

```{r dists}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Kernel PCA - distance to each centroid"
dist_to_classes <- basic %>% 
  recipes::step_classdist(recipes::all_predictors(), class = "Class") %>%
  # Take log of the new distance features
  recipes::step_log(starts_with("classdist"))

dist_to_classes <- recipes::prep(dist_to_classes, verbose = FALSE)

# All variables are retained plus an additional one for each class
dist_to_classes <- recipes::bake(dist_to_classes, new_data = seg_test, matches("[Cc]lass"))
dist_to_classes
```

## Distance to Each Class

```{r image_dists_fig}
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Kernel PCA - distance to each class"
#| width: "80%"
rngs <- extendrange(c(dist_to_classes$classdist_PS, dist_to_classes$classdist_WS))
ggplot(dist_to_classes, aes(x = classdist_PS, y = classdist_WS, color = Class)) + 
  geom_point(alpha = .4) + 
  xlim(rngs) + ylim(rngs) + 
  # theme(legend.position = "top") + 
  theme_bw(base_size = 16) +
  xlab("Distance to PS Centroid (log scale)") + 
  ylab("Distance to WS Centroid (log scale)")
```

## Recap

-   We've used the Recipes package to create a workflow for data pre-processing and feature engineering
-   The verb `recipe` defines the pre-processing and feature engineering steps
-   using the recipe object, the verb `prep` prepares the data on a training set, storing the metaparameters.
-   the verb bake applies the prepped recipe to new data, using the metaparameters.

------------------------------------------------------------------------

```{r, echo = FALSE}
sessionInfo()
```
